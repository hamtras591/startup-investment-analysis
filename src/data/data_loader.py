"""
Universal Data Loader Module
============================
M√≥dulo reutilizable para cargar cualquier tipo de datos sin problemas de encoding.

PROP√ìSITO:
    Cargar y guardar datos en m√∫ltiples formatos (CSV, Excel, JSON, Parquet)
    con detecci√≥n autom√°tica de encoding y manejo robusto de errores.

FUNCIONALIDADES:
    1. Detecci√≥n autom√°tica de encoding (UTF-8, Latin-1, etc.)
    2. Detecci√≥n autom√°tica de delimitadores (CSV)
    3. Soporte para m√∫ltiples formatos
    4. Cach√© en memoria para archivos cargados
    5. Validaci√≥n de corrupci√≥n de caracteres
    6. Integraci√≥n con config.py para rutas estandarizadas

UBICACI√ìN EN EL PROYECTO:
    src/data/data_loader.py

AUTOR: Anderson Sebastian Rubio Pacheco
VERSI√ìN: 2.0.0
FECHA: Octubre 2025
CAMBIOS v2.0.0: Integraci√≥n completa con config.py
"""

# ============================================================
# SECCI√ìN 1: IMPORTACIONES
# ============================================================

import pandas as pd  # Manipulaci√≥n de datos
from pathlib import Path  # Manejo moderno de rutas
import chardet  # Detecci√≥n de encoding
import logging  # Sistema de logs
from typing import Optional, Dict, Any, Union

# ============================================================
# Importar configuraci√≥n centralizada del proyecto
# ============================================================
try:
    # Importar rutas desde config.py
    from src.data.config import (
        RAW_DATA_DIRECTORY,  # data/raw/
        PROCESSED_DATA_DIRECTORY  # data/processed/
    )

    CONFIG_AVAILABLE = True
except ImportError:
    # Fallback si config.py no est√° disponible
    RAW_DATA_DIRECTORY = Path('./data/raw/')
    PROCESSED_DATA_DIRECTORY = Path('./data/processed/')
    CONFIG_AVAILABLE = False

# ============================================================
# Configurar logging
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================
# CLASE PRINCIPAL: UniversalDataLoader
# ============================================================

class UniversalDataLoader:
    """
    Cargador universal de datos con detecci√≥n autom√°tica

    RESPONSABILIDADES:
        - Detectar encoding autom√°ticamente
        - Detectar delimitadores en CSVs
        - Cargar m√∫ltiples formatos (CSV, Excel, JSON, Parquet)
        - Validar integridad de datos
        - Cachear archivos en memoria
        - Guardar datos en formatos limpios

    FORMATOS SOPORTADOS:
        - CSV (.csv, .txt, .tsv)
        - Excel (.xlsx, .xls)
        - JSON (.json)
        - Parquet (.parquet)

    ATRIBUTOS:
        verbose (bool): Controla mensajes detallados
        _cache (dict): Cach√© de DataFrames cargados

    EJEMPLO DE USO:
        >>> loader = UniversalDataLoader()
        >>> df = loader.load('data.csv')
        >>> loader.save_data(df, 'clean_data.csv', location='processed')
    """

    # ========================================================
    # M√âTODO: __init__ (Constructor)
    # ========================================================
    def __init__(self, verbose: bool = True):
        """
        Inicializa el cargador universal

        Args:
            verbose: Si True, muestra mensajes detallados del proceso
        """
        self.verbose = verbose  # Controla verbosidad
        self._cache = {}  # Diccionario para cachear DataFrames

    # ========================================================
    # M√âTODO PRIVADO: _log
    # ========================================================
    def _log(self, message: str, level=logging.INFO):
        """
        Registra mensaje en el logger si verbose est√° activo

        Args:
            message: Mensaje a mostrar
            level: Nivel de logging (INFO, WARNING, ERROR)
        """
        if self.verbose:
            logger.log(level, message)

    # ========================================================
    # M√âTODOS DE DETECCI√ìN
    # ========================================================

    def _detect_encoding(self, filepath: Path, sample_size: int = 100000) -> tuple:
        """
        Detecta el encoding del archivo usando chardet

        ESTRATEGIA:
            1. Lee los primeros N bytes del archivo
            2. Usa chardet para detectar el encoding
            3. Retorna encoding y nivel de confianza

        Args:
            filepath: Ruta del archivo a analizar
            sample_size: N√∫mero de bytes a leer para la muestra

        Returns:
            tuple: (encoding, confianza)
                  Ejemplo: ('utf-8', 0.99)
        """
        try:
            # Abrir archivo en modo binario
            with open(filepath, 'rb') as file:
                # Leer muestra de bytes
                raw_data = file.read(sample_size)

                # Detectar encoding con chardet
                result = chardet.detect(raw_data)
                # result = {'encoding': 'utf-8', 'confidence': 0.99, ...}

                return result['encoding'], result['confidence']

        except Exception:
            # Si falla la detecci√≥n, usar UTF-8 por defecto
            self._log("‚ö†Ô∏è Fallback: Usando utf-8 por defecto.", logging.WARNING)
            return 'utf-8', 0.5

    def _detect_delimiter(self, filepath: Path) -> str:
        """
        Detecta el delimitador del archivo CSV

        ESTRATEGIA:
            1. Lee la primera l√≠nea del archivo
            2. Cuenta ocurrencias de delimitadores comunes
            3. Retorna el m√°s frecuente

        DELIMITADORES COMUNES:
            - ',' (comma) - Est√°ndar internacional
            - ';' (semicolon) - Com√∫n en Europa/Latinoam√©rica
            - '\t' (tab) - TSV (Tab-Separated Values)
            - '|' (pipe) - Usado en bases de datos

        Args:
            filepath: Ruta del archivo CSV

        Returns:
            str: Delimitador detectado (',', ';', '\t', '|')
        """
        # Abrir archivo con encoding tolerante a errores
        with open(filepath, 'r', encoding='latin-1', errors='ignore') as file:
            first_line = file.readline()  # Leer primera l√≠nea

        # Contar ocurrencias de cada delimitador
        delimiters = {
            ',': first_line.count(','),  # Comas
            ';': first_line.count(';'),  # Punto y coma
            '\t': first_line.count('\t'),  # Tabulaciones
            '|': first_line.count('|')  # Pipes
        }

        # Retornar el delimitador con m√°s ocurrencias
        return max(delimiters, key=delimiters.get)
        # max(..., key=...) encuentra la clave con el valor m√°ximo

    def _check_corruption(self, df: pd.DataFrame) -> bool:
        """
        Verifica si hay caracteres corruptos en el DataFrame

        PATRONES DE CORRUPCI√ìN COMUNES:
            Cuando un archivo UTF-8 se lee como Latin-1:
            - '√±' se corrompe a: '√É¬±'
            - '√°' se corrompe a: '√É¬°'
            - '√©' se corrompe a: '√É¬©'
            - '¬£' se corrompe a: '√Ç¬£'

        Args:
            df: DataFrame a verificar

        Returns:
            bool: True si se detecta corrupci√≥n, False si est√° limpio
        """
        # Patrones que indican corrupci√≥n de encoding
        corruption_patterns = ['√É', '√Ç¬£', '√É¬±', '√É¬°', '√É¬©', '√Ç']

        # Revisar solo columnas de texto (object dtype)
        for col in df.select_dtypes(include=['object']).columns:
            # Tomar muestra de las primeras 100 filas
            sample = df[col].astype(str).head(100).str.cat(sep=' ')
            # .str.cat(sep=' ') concatena todas las filas con espacio

            # Verificar si alg√∫n patr√≥n de corrupci√≥n est√° presente
            if any(pattern in sample for pattern in corruption_patterns):
                return True  # Corrupci√≥n detectada

        return False  # No hay corrupci√≥n

    # ========================================================
    # M√âTODOS DE CARGA POR FORMATO
    # ========================================================

    def _load_csv(self, filepath: Path, **kwargs) -> pd.DataFrame:
        """
        Carga archivos CSV con detecci√≥n autom√°tica

        PROCESO:
            1. Detectar delimitador si no se especific√≥
            2. Detectar encoding si no se especific√≥
            3. Intentar cargar con configuraci√≥n detectada
            4. Si falla, usar fallback robusto
            5. Validar corrupci√≥n de caracteres

        Args:
            filepath: Ruta del archivo CSV
            **kwargs: Argumentos adicionales para pd.read_csv()

        Returns:
            pd.DataFrame: Datos cargados
        """
        # ----------------------------------------------------
        # 1. Detectar separador si no se especific√≥
        # ----------------------------------------------------
        if 'sep' not in kwargs and 'delimiter' not in kwargs:
            kwargs['sep'] = self._detect_delimiter(filepath)
            self._log(f"üìä Separador detectado: '{kwargs['sep']}'")

        # ----------------------------------------------------
        # 2. Detectar encoding si no se especific√≥
        # ----------------------------------------------------
        if 'encoding' not in kwargs:
            encoding, confidence = self._detect_encoding(filepath)
            kwargs['encoding'] = encoding
            self._log(f"üî§ Encoding detectado: {encoding} ({confidence:.1%} confianza)")

        # ----------------------------------------------------
        # 3. Intentar cargar con configuraci√≥n detectada
        # ----------------------------------------------------
        try:
            df = pd.read_csv(filepath, **kwargs)
            self._log(f"‚úÖ CSV cargado: {df.shape[0]:,} filas √ó {df.shape[1]} columnas")

        # ----------------------------------------------------
        # 4. Manejo de errores comunes
        # ----------------------------------------------------
        except UnicodeDecodeError:
            # Error de encoding: intentar con Latin-1
            self._log("‚ö†Ô∏è Error de encoding, intentando con latin-1")
            kwargs['encoding'] = 'latin-1'
            df = pd.read_csv(filepath, **kwargs)

        except Exception:
            # Cualquier otro error: usar m√©todo de respaldo
            df = self._fallback_csv_load(filepath)

        # ----------------------------------------------------
        # 5. Validar integridad de los datos
        # ----------------------------------------------------
        if self._check_corruption(df):
            self._log("‚ö†Ô∏è Posible corrupci√≥n de caracteres detectada", logging.WARNING)

        return df

    def _load_excel(self, filepath: Path, **kwargs) -> pd.DataFrame:
        """
        Carga archivos Excel (.xlsx, .xls)

        Args:
            filepath: Ruta del archivo Excel
            **kwargs: Argumentos para pd.read_excel()
                     (sheet_name, usecols, etc.)

        Returns:
            pd.DataFrame: Datos cargados
        """
        try:
            df = pd.read_excel(filepath, **kwargs)
            self._log(f"‚úÖ Excel cargado: {df.shape[0]:,} filas √ó {df.shape[1]} columnas")
            return df
        except Exception as e:
            self._log(f"‚ùå Error cargando Excel: {e}", logging.ERROR)
            raise

    def _load_json(self, filepath: Path, **kwargs) -> pd.DataFrame:
        """
        Carga archivos JSON con detecci√≥n de orientaci√≥n

        ORIENTACIONES SOPORTADAS:
            - 'records': [{col1: val, col2: val}, ...]
            - 'index': {index: {col: val, ...}, ...}
            - 'columns': {col: {index: val, ...}, ...}
            - 'values': [[val1, val2, ...], ...]

        Args:
            filepath: Ruta del archivo JSON
            **kwargs: Argumentos para pd.read_json()

        Returns:
            pd.DataFrame: Datos cargados
        """
        try:
            # Intentar con configuraci√≥n proporcionada
            df = pd.read_json(filepath, **kwargs)
            self._log(f"‚úÖ JSON cargado: {df.shape[0]:,} filas √ó {df.shape[1]} columnas")
            return df

        except Exception as e:
            # Si falla, probar diferentes orientaciones
            self._log("‚ö†Ô∏è Intentando con orientaciones diferentes para JSON")

            for orient in ['records', 'index', 'columns', 'values']:
                try:
                    df = pd.read_json(filepath, orient=orient)
                    self._log(f"‚úÖ JSON cargado con orient='{orient}'")
                    return df
                except:
                    continue  # Probar siguiente orientaci√≥n

            # Si ninguna funcion√≥, lanzar el error original
            self._log(f"‚ùå Fall√≥ la carga de JSON: {e}", logging.ERROR)
            raise

    def _load_parquet(self, filepath: Path, **kwargs) -> pd.DataFrame:
        """
        Carga archivos Parquet (formato columnar eficiente)

        VENTAJAS DE PARQUET:
            - Compresi√≥n eficiente (archivos m√°s peque√±os)
            - Lectura r√°pida (formato columnar)
            - Preserva tipos de datos
            - Soporta esquemas complejos

        REQUISITO: pip install pyarrow

        Args:
            filepath: Ruta del archivo Parquet
            **kwargs: Argumentos para pd.read_parquet()

        Returns:
            pd.DataFrame: Datos cargados
        """
        try:
            df = pd.read_parquet(filepath, **kwargs)
            self._log(f"‚úÖ Parquet cargado: {df.shape[0]:,} filas √ó {df.shape[1]} columnas")
            return df
        except ImportError:
            self._log("‚ùå Instala 'pyarrow' para leer Parquet", logging.ERROR)
            raise

    def _fallback_csv_load(self, filepath: Path) -> pd.DataFrame:
        """
        M√©todo de respaldo para CSVs problem√°ticos

        CONFIGURACI√ìN ROBUSTA:
            - encoding='latin-1': Encoding tolerante
            - sep=None: Detecci√≥n autom√°tica de separador
            - engine='python': Motor m√°s flexible (no C)
            - on_bad_lines='skip': Ignora l√≠neas malformadas
            - encoding_errors='ignore': Ignora errores de encoding

        Args:
            filepath: Ruta del archivo CSV problem√°tico

        Returns:
            pd.DataFrame: Datos cargados (puede tener filas faltantes)
        """
        self._log("‚ö†Ô∏è Usando configuraci√≥n robusta de fallback")

        return pd.read_csv(
            filepath,
            encoding='latin-1',  # Encoding tolerante
            sep=None,  # Auto-detectar separador
            engine='python',  # Motor flexible
            on_bad_lines='skip',  # Omitir l√≠neas con errores
            encoding_errors='ignore'  # Ignorar errores de encoding
        )

    # ========================================================
    # M√âTODO PRINCIPAL: load
    # ========================================================

    def load(self,
             filepath: Union[str, Path],
             force_reload: bool = False,
             **kwargs) -> pd.DataFrame:
        """
        M√©todo principal - carga cualquier archivo autom√°ticamente

        FLUJO DE CARGA:
            1. Verificar cach√© (si force_reload=False)
            2. Detectar tipo de archivo por extensi√≥n
            3. Llamar al m√©todo de carga apropiado
            4. Guardar en cach√©
            5. Agregar metadata al DataFrame
            6. Retornar DataFrame

        Args:
            filepath: Ruta del archivo (str o Path)
            force_reload: Si True, ignora cach√© y recarga
            **kwargs: Argumentos espec√≠ficos del formato

        Returns:
            pd.DataFrame: Datos cargados con metadata

        Raises:
            ValueError: Si el formato no es soportado

        Ejemplo:
            >>> loader = UniversalDataLoader()
            >>> df = loader.load('data.csv')
            >>> df = loader.load('data.xlsx', sheet_name='Hoja1')
            >>> df = loader.load('data.json', orient='records')
        """
        # ----------------------------------------------------
        # 1. Convertir a Path para operaciones modernas
        # ----------------------------------------------------
        filepath = Path(filepath)

        # ----------------------------------------------------
        # 2. Verificar cach√©
        # ----------------------------------------------------
        cache_key = str(filepath.absolute())
        # .absolute() da la ruta absoluta completa

        if not force_reload and cache_key in self._cache:
            self._log("üì¶ Cargando desde cach√©")
            return self._cache[cache_key]

        # ----------------------------------------------------
        # 3. Detectar tipo de archivo y cargar
        # ----------------------------------------------------
        file_extension = filepath.suffix.lower()
        # .suffix obtiene la extensi√≥n: 'archivo.csv' ‚Üí '.csv'
        # .lower() normaliza a min√∫sculas

        # Delegar a m√©todo espec√≠fico seg√∫n extensi√≥n
        if file_extension in ['.csv', '.txt', '.tsv']:
            df = self._load_csv(filepath, **kwargs)

        elif file_extension in ['.xlsx', '.xls']:
            df = self._load_excel(filepath, **kwargs)

        elif file_extension == '.json':
            df = self._load_json(filepath, **kwargs)

        elif file_extension == '.parquet':
            df = self._load_parquet(filepath, **kwargs)

        else:
            # Formato no reconocido
            raise ValueError(f"Formato no soportado: {file_extension}")

        # ----------------------------------------------------
        # 4. Guardar en cach√©
        # ----------------------------------------------------
        self._cache[cache_key] = df

        # ----------------------------------------------------
        # 5. Agregar metadata al DataFrame
        # ----------------------------------------------------
        df.attrs['source_file'] = str(filepath)
        # .attrs es un diccionario de metadata del DataFrame
        df.attrs['load_timestamp'] = pd.Timestamp.now()

        return df

    # ========================================================
    # M√âTODO: save_data
    # ‚úÖ INTEGRADO CON config.py
    # ========================================================

    def save_data(self,
                  df: pd.DataFrame,
                  filename: str,
                  location: str = 'processed',
                  format: str = 'auto',
                  index: bool = False) -> Path:
        """
        Guarda DataFrame usando rutas de config.py

        UBICACIONES V√ÅLIDAS:
            - 'raw': data/raw/ (RAW_DATA_DIRECTORY)
            - 'processed': data/processed/ (PROCESSED_DATA_DIRECTORY)

        FORMATOS SOPORTADOS:
            - 'csv': CSV con UTF-8
            - 'excel': Excel (.xlsx)
            - 'parquet': Parquet (requiere pyarrow)
            - 'auto': Detectar por extensi√≥n

        Args:
            df: DataFrame a guardar
            filename: Nombre del archivo (con extensi√≥n)
            location: 'raw' o 'processed'
            format: Formato de guardado
            index: Incluir √≠ndice del DataFrame

        Returns:
            Path: Ruta donde se guard√≥ el archivo

        Ejemplo:
            >>> loader = UniversalDataLoader()
            >>> loader.save_data(df, 'clean.csv', location='processed')
            >>> loader.save_data(df, 'backup.parquet', location='raw')
        """
        # ----------------------------------------------------
        # 1. Determinar directorio usando config.py
        # ‚úÖ CAMBIO v2.0.0: Usar variables importadas
        # ----------------------------------------------------
        if location.lower() == 'raw':
            target_dir = RAW_DATA_DIRECTORY
        elif location.lower() == 'processed':
            target_dir = PROCESSED_DATA_DIRECTORY
        else:
            raise ValueError("location debe ser 'raw' o 'processed'")

        # Construir ruta completa
        output_path = target_dir / filename

        # ----------------------------------------------------
        # 2. Detectar formato si es 'auto'
        # ----------------------------------------------------
        if format == 'auto':
            # Usar la extensi√≥n del archivo
            format = output_path.suffix[1:] if output_path.suffix else 'csv'
            # .suffix[1:] elimina el punto: '.csv' ‚Üí 'csv'

        # ----------------------------------------------------
        # 3. Mensaje informativo
        # ----------------------------------------------------
        self._log(f"üíæ Guardando en {target_dir.name}/{filename} como {format.upper()}")

        # ----------------------------------------------------
        # 4. Guardar seg√∫n formato
        # ----------------------------------------------------
        if format == 'csv':
            df.to_csv(output_path, encoding='utf-8', index=index)

        elif format == 'excel':
            df.to_excel(output_path, index=index)

        elif format == 'parquet':
            df.to_parquet(output_path, index=index)

        else:
            raise ValueError(f"Formato no soportado: {format}")

        self._log(f"‚úÖ Guardado exitoso: {output_path}")
        return output_path


# ============================================================
# FUNCIONES DE CONVENIENCIA (Shortcuts)
# ============================================================

def load_raw(filename: str, **kwargs) -> pd.DataFrame:
    """
    Carga archivo desde data/raw/ con b√∫squeda recursiva

    B√öSQUEDA RECURSIVA:
        Busca el archivo en data/raw/ y todas sus subcarpetas.
        Ejemplo:
            data/raw/
            ‚îú‚îÄ‚îÄ archivo.csv          ‚Üê Encuentra esto
            ‚îú‚îÄ‚îÄ carpeta1/
            ‚îÇ   ‚îî‚îÄ‚îÄ archivo.csv      ‚Üê O esto
            ‚îî‚îÄ‚îÄ carpeta2/
                ‚îî‚îÄ‚îÄ sub/
                    ‚îî‚îÄ‚îÄ archivo.csv  ‚Üê O esto

    Args:
        filename: Nombre exacto del archivo
        **kwargs: Argumentos para el loader

    Returns:
        pd.DataFrame: Datos cargados

    Raises:
        FileNotFoundError: Si el archivo no existe

    Ejemplo:
        >>> from src.data.data_loader import load_raw
        >>> df = load_raw('investments.csv')
        >>> df = load_raw('data.xlsx', sheet_name='Sheet1')
    """
    # ----------------------------------------------------
    # 1. Crear instancia del loader
    # ----------------------------------------------------
    loader = UniversalDataLoader()

    # ----------------------------------------------------
    # 2. Buscar archivo recursivamente
    # ‚úÖ USA RAW_DATA_DIRECTORY de config.py
    # ----------------------------------------------------
    search_results = list(RAW_DATA_DIRECTORY.rglob(filename))
    # .rglob() busca recursivamente (r = recursive)
    # Ejemplo: RAW_DATA_DIRECTORY.rglob('*.csv') busca todos los CSVs

    # ----------------------------------------------------
    # 3. Validar resultados
    # ----------------------------------------------------
    if not search_results:
        # Archivo no encontrado
        raise FileNotFoundError(
            f"‚ùå Archivo '{filename}' no encontrado en {RAW_DATA_DIRECTORY}\n"
            f"   Buscar en subdirectorios tambi√©n.\n"
            f"   Verifica el nombre exacto del archivo."
        )

    # ----------------------------------------------------
    # 4. Usar primer resultado encontrado
    # ----------------------------------------------------
    filepath = search_results[0]

    # ----------------------------------------------------
    # 5. Advertir si hay m√∫ltiples coincidencias
    # ----------------------------------------------------
    if len(search_results) > 1:
        logging.warning(
            f"‚ö†Ô∏è M√∫ltiples archivos '{filename}' encontrados. "
            f"Usando: {filepath.relative_to(RAW_DATA_DIRECTORY)}"
        )
        # .relative_to() muestra ruta relativa: data/raw/carpeta/archivo.csv ‚Üí carpeta/archivo.csv

    # ----------------------------------------------------
    # 6. Cargar y retornar
    # ----------------------------------------------------
    logging.info(f"üìÅ Cargando: {filepath.relative_to(RAW_DATA_DIRECTORY)}")
    return loader.load(filepath, **kwargs)


def save_processed(df: pd.DataFrame, filename: str, **kwargs) -> Path:
    """
    Guarda DataFrame en data/processed/

    Args:
        df: DataFrame a guardar
        filename: Nombre del archivo
        **kwargs: Argumentos para save_data()

    Returns:
        Path: Ruta donde se guard√≥

    Ejemplo:
        >>> from src.data.data_loader import save_processed
        >>> save_processed(df_clean, 'clean_data.csv')
        >>> save_processed(df_clean, 'backup.parquet')
    """
    loader = UniversalDataLoader()
    return loader.save_data(df, filename, location='processed', **kwargs)


def load_and_clean(filepath: Union[str, Path]) -> pd.DataFrame:
    """
    Carga y limpia autom√°ticamente

    LIMPIEZA AUTOM√ÅTICA:
        1. Elimina filas completamente vac√≠as
        2. Elimina columnas completamente vac√≠as
        3. Elimina duplicados exactos

    Args:
        filepath: Ruta del archivo

    Returns:
        pd.DataFrame: Datos cargados y limpiados

    Ejemplo:
        >>> from src.data.data_loader import load_and_clean
        >>> df = load_and_clean('messy_data.csv')
    """
    loader = UniversalDataLoader(verbose=True)
    df = loader.load(filepath)

    # Guardar shape inicial
    initial_shape = df.shape

    # Limpieza autom√°tica b√°sica
    df = df.dropna(how='all')  # Eliminar filas vac√≠as
    df = df.dropna(axis=1, how='all')  # Eliminar columnas vac√≠as
    df = df.drop_duplicates()  # Eliminar duplicados

    final_shape = df.shape

    # Mostrar resumen de limpieza
    if initial_shape != final_shape:
        print(f"üßπ Limpieza: {initial_shape} ‚Üí {final_shape}")
        rows_removed = initial_shape[0] - final_shape[0]
        cols_removed = initial_shape[1] - final_shape[1]
        print(f"   Filas eliminadas: {rows_removed}")
        print(f"   Columnas eliminadas: {cols_removed}")

    return df


# ============================================================
# EXPORTACIONES (para imports limpios)
# ============================================================
__all__ = [
    'UniversalDataLoader',  # Clase principal
    'load_raw',  # Cargar desde data/raw/
    'save_processed',  # Guardar en data/processed/
    'load_and_clean'  # Cargar + limpiar
]